{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fec07a8",
   "metadata": {},
   "source": [
    "### This files has RAG full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae850c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'RAG (3.13.5) (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/yella/OneDrive/Desktop/RAG/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "### Creating the txt files from txt data\n",
    "text=['''Trees are the foundation of life on Earth, providing oxygen, shelter, and food for countless species. They absorb carbon dioxide, helping to fight climate change, and regulate temperatures through shade and moisture. Forests act as natural habitats for animals and serve as barriers against soil erosion. Beyond their ecological role, trees also \n",
    "      bring peace and beauty to human surroundings, reminding us of the importance of balance in nature.''',\n",
    "\n",
    "      '''Animals play a vital role in maintaining ecological balance by forming interconnected food chains. From insects that pollinate crops to predators that control populations, each species contributes to the health of ecosystems. They also provide companionship, work assistance, and inspiration to humans. Protecting wildlife is essential,\n",
    "        not only for biodiversity but also for ensuring the stability of natural environments that sustain all life.''',\n",
    "\n",
    "        '''Cars revolutionized human mobility by offering independence and speed, connecting people and places like never before. They have become an essential part of modern life, driving economic growth and shaping urban design. Over the years, advancements like electric vehicles and autonomous driving have made cars smarter and cleaner. Yet, the challenge remains\n",
    "          to balance convenience with sustainability and reduce the environmental impact of transportation.''',\n",
    "\n",
    "          '''Petrol, derived from crude oil, has long been the main energy source for vehicles and industries. It powers engines efficiently but also contributes to pollution and greenhouse gas emissions. The global demand for petrol has driven innovation in energy exploration and production. However, with rising environmental concerns, \n",
    "          the world is gradually shifting toward renewable alternatives like solar and electric energy to ensure a cleaner future.''',\n",
    "\n",
    "          '''Fire is both a life-giving and destructive force. It provides warmth, light, and a means to cook food, marking one of humanity’s greatest discoveries. However, when uncontrolled, fire can cause immense damage to forests, homes, and lives. In nature, fire also plays a role in regeneration, helping certain plants release seeds and recycle nutrients.\n",
    "            Understanding and respecting fire’s dual nature is key to using it safely and wisely.'''\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.makedirs(\"textfiles\",exist_ok=True)\n",
    "\n",
    "for i,text_sent in enumerate(text):\n",
    "    with open(f'textfiles/doc{i}.txt','w',encoding=\"utf-8\") as f:\n",
    "        f.write(text_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now time to make documents,page_content\n",
    "\n",
    "loader=DirectoryLoader(\n",
    "    \"textfiles\",\n",
    "    glob=\"*.txt\",\n",
    "    load_hidden=TextLoader,\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"}\n",
    "\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0073e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=loader.load()\n",
    "\n",
    "\n",
    "for i,document in enumerate(docs):\n",
    "    print(f'this is document {i+1} ')\n",
    "    print(document)\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ddce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We have list of documents so now we have to make chunks\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=2,\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=splitter.split_documents(docs)\n",
    "chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc82aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize vector store\n",
    "\n",
    "## creating vectore store folder\n",
    "\n",
    "\n",
    "'''from langchain.vectorstores import Chroma\n",
    "os.makedirs(\"vec_dir\",exist_ok=True)\n",
    "\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"vec_dir\",\n",
    "    collection_name=\"RAG\"\n",
    ")\n",
    "\n",
    "'''\n",
    "import os, json, hashlib\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document  # just for type hints\n",
    "\n",
    "# ---- 1) init / reopen existing collection ----\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"vec_dir\",\n",
    "    collection_name=\"RAG\",\n",
    "    embedding_function=embeddings,   # your embedding function\n",
    ")\n",
    "\n",
    "# ---- 2) make a stable ID per chunk (hash of content + key metadata) ----\n",
    "def make_id(doc: Document) -> str:\n",
    "    payload = {\n",
    "        \"text\": doc.page_content.strip(),\n",
    "        \"source\": doc.metadata.get(\"source\"),\n",
    "        \"page\": doc.metadata.get(\"page\"),\n",
    "    }\n",
    "    s = json.dumps(payload, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# If you create chunks elsewhere, ensure each has doc.metadata[\"source\"]/[\"page\"] if available.\n",
    "pairs = [(make_id(d), d) for d in chunks]\n",
    "\n",
    "# local de-dup in case the same text appears twice in `chunks` this run\n",
    "unique = {}\n",
    "for _id, d in pairs:\n",
    "    if _id not in unique:\n",
    "        unique[_id] = d\n",
    "ids = list(unique.keys())\n",
    "docs = list(unique.values())\n",
    "\n",
    "# ---- 3) add only missing IDs ----\n",
    "# try efficient filtered get; fall back to full get if needed (older versions)\n",
    "try:\n",
    "    existing = set(vectorstore.get(ids=ids, include=[]).get(\"ids\", []))\n",
    "except Exception:\n",
    "    existing = set(vectorstore.get(include=[]).get(\"ids\", []))\n",
    "\n",
    "to_add_ids  = [i for i in ids if i not in existing]\n",
    "to_add_docs = [d for i, d in zip(ids, docs) if i not in existing]\n",
    "\n",
    "if to_add_docs:\n",
    "    vectorstore.add_documents(to_add_docs, ids=to_add_ids)\n",
    "    vectorstore.persist()\n",
    "\n",
    "# ---- query as usual ----\n",
    "# results = vectorstore.similarity_search(\"how houses are made\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654a0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"how houses are made\"\n",
    "output=vectorstore.similarity_search(query,k=3)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef041789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20436c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"]=os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def pip_install(args):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args])\n",
    "\n",
    "# Ensure dependencies are available\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "except Exception:\n",
    "    pip_install([\"transformers\", \"accelerate\"])  # lightweight helper libs\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    # Install CPU-only PyTorch wheels\n",
    "    pip_install([\"torch\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
    "    import torch\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Use a small, fully open-source chat model that runs on CPU\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  # CPU-friendly\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.2,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(llm.invoke(\"color of trees\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first covert vectorestore to retriver\n",
    "\n",
    "retriver=vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\":3} #top k chunks to be retrived\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5561ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating prompt templete to instruct llms\n",
    "\n",
    "system_prompt=\"\"\"Act as a assistant for question and answering\n",
    "use the following context for answering the questions.\n",
    "if you dont know the answer say that you dont know.\n",
    "use these sentances and make the answers consise.\n",
    "\n",
    "context : {context}\n",
    "\"\"\"\n",
    " \n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "  (\"system\",system_prompt),\n",
    "  (\"human\",\"{input}\")\n",
    " ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create document chain which combines all the retrived chunks and provide to llm\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain=create_retrieval_chain(retriver,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=rag_chain.invoke({\"input\":\"question for llm\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab5af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
