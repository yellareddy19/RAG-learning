{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650830dc",
   "metadata": {},
   "source": [
    "### welcome to RAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc3089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict,Any\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ae2476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\RAG\\\\.venv\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f64ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=Document(\n",
    "    page_content=\"hi this is yella reddy learning RAG\",\n",
    "    metadata={\n",
    "        \"Name\":\"Yella reddy\",\n",
    "        \"Date\":\"19/09/2025\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adafd97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi this is yella reddy learning RAG\n",
      "{'Name': 'Yella reddy', 'Date': '19/09/2025'}\n"
     ]
    }
   ],
   "source": [
    "print(doc.page_content)\n",
    "print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7cc388",
   "metadata": {},
   "source": [
    "### Creating text files using OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed735451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b50defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text={\n",
    "    \"data/text_files/ml.txt\":\"\"\" machine learning is making the machines to learn itself. \n",
    "    Machine Learning (ML) is a branch of Artificial Intelligence that focuses on building systems capable of learning patterns from data and making predictions or decisions without being explicitly programmed. It involves training algorithms on historical datasets so they can identify trends, classify information, or forecast future outcomes. ML techniques range from simple models like linear regression to complex neural networks that power modern applications such as speech recognition, recommendation systems, fraud detection, and medical diagnostics. By continuously improving with more data, \n",
    "    ML enables businesses and researchers to uncover insights, automate processes, and solve problems at scale.\"\"\",\n",
    "\n",
    "    \"data/text_files/rag.txt\":\"\"\"Retrieval-Augmented Generation (RAG) is an advanced framework in artificial intelligence that combines information retrieval with generative modeling to produce more accurate and contextually relevant responses.\n",
    "It bridges the gap between traditional search systems and generative models by integrating external knowledge sources into the generation process.\n",
    "Instead of relying only on pre-trained parameters of a large language model, RAG actively queries a knowledge base or vector database during inference.\n",
    "This allows it to bring in the most up-to-date and domain-specific information that the model may not have memorized.\n",
    "The architecture typically involves two main components: a retriever and a generator.\n",
    "The retriever searches external documents, embeddings, or indexed corpora to fetch the most relevant passages.\n",
    "The generator then conditions on both the query and the retrieved content to create a coherent and factually grounded answer.\n",
    "This process ensures that the model is less prone to hallucinations and misinformation.\n",
    "RAG is widely used in building enterprise chatbots, customer support systems, and domain-specific assistants.\n",
    "For example, in healthcare, it can access electronic health records or medical literature to provide compliance-friendly and accurate responses.\n",
    "In finance, it can pull the latest regulations or market data before generating recommendations.\n",
    "The retriever can be powered by dense vector embeddings using models like DPR or sentence-transformers.\n",
    "The generator is often a large language model such as BERT, GPT, or T5 adapted for conditioned text generation.\n",
    "RAG also supports multi-turn conversations by maintaining retrieval memory across queries.\n",
    "Its modular design allows organizations to plug in different retrieval systems or update knowledge bases without retraining the generator.\n",
    "This makes RAG more scalable and adaptive compared to static models.\n",
    "It also enhances transparency, as the retrieved sources can be displayed to justify the generated outputs.\n",
    "Recent advancements combine RAG with multi-agent systems and LangChain pipelines for orchestration.\n",
    "Overall, RAG represents a key step toward trustworthy, knowledge-grounded AI.\n",
    "It enables real-world deployment of generative models where accuracy, explainability, and compliance are critical.\n",
    "\n",
    "Would you like me to also **convert this into a simpler 5–6 line version** for quick interview use?\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a49ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files created succesfully\n"
     ]
    }
   ],
   "source": [
    "for filepath,text in sample_text.items():\n",
    "    with open(filepath,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(\"files created succesfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b162e5",
   "metadata": {},
   "source": [
    "## Text loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "176ddf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data/text_files/rag.txt'}, page_content='Retrieval-Augmented Generation (RAG) is an advanced framework in artificial intelligence that combines information retrieval with generative modeling to produce more accurate and contextually relevant responses.\\nIt bridges the gap between traditional search systems and generative models by integrating external knowledge sources into the generation process.\\nInstead of relying only on pre-trained parameters of a large language model, RAG actively queries a knowledge base or vector database during inference.\\nThis allows it to bring in the most up-to-date and domain-specific information that the model may not have memorized.\\nThe architecture typically involves two main components: a retriever and a generator.\\nThe retriever searches external documents, embeddings, or indexed corpora to fetch the most relevant passages.\\nThe generator then conditions on both the query and the retrieved content to create a coherent and factually grounded answer.\\nThis process ensures that the model is less prone to hallucinations and misinformation.\\nRAG is widely used in building enterprise chatbots, customer support systems, and domain-specific assistants.\\nFor example, in healthcare, it can access electronic health records or medical literature to provide compliance-friendly and accurate responses.\\nIn finance, it can pull the latest regulations or market data before generating recommendations.\\nThe retriever can be powered by dense vector embeddings using models like DPR or sentence-transformers.\\nThe generator is often a large language model such as BERT, GPT, or T5 adapted for conditioned text generation.\\nRAG also supports multi-turn conversations by maintaining retrieval memory across queries.\\nIts modular design allows organizations to plug in different retrieval systems or update knowledge bases without retraining the generator.\\nThis makes RAG more scalable and adaptive compared to static models.\\nIt also enhances transparency, as the retrieved sources can be displayed to justify the generated outputs.\\nRecent advancements combine RAG with multi-agent systems and LangChain pipelines for orchestration.\\nOverall, RAG represents a key step toward trustworthy, knowledge-grounded AI.\\nIt enables real-world deployment of generative models where accuracy, explainability, and compliance are critical.\\n\\nWould you like me to also **convert this into a simpler 5–6 line version** for quick interview use?\\n')]\n",
      "this id {'source': 'data/text_files/rag.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doc=TextLoader(\"data/text_files/rag.txt\",encoding=\"utf-8\")\n",
    "document=doc.load()\n",
    "print(document)\n",
    "print(f\"this id {document[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d8537",
   "metadata": {},
   "source": [
    "### Directory loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e134d61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 585.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\text_files\\\\ml.txt'}, page_content=' machine learning is making the machines to learn itself. \\n    Machine Learning (ML) is a branch of Artificial Intelligence that focuses on building systems capable of learning patterns from data and making predictions or decisions without being explicitly programmed. It involves training algorithms on historical datasets so they can identify trends, classify information, or forecast future outcomes. ML techniques range from simple models like linear regression to complex neural networks that power modern applications such as speech recognition, recommendation systems, fraud detection, and medical diagnostics. By continuously improving with more data, \\n    ML enables businesses and researchers to uncover insights, automate processes, and solve problems at scale.'), Document(metadata={'source': 'data\\\\text_files\\\\rag.txt'}, page_content='Retrieval-Augmented Generation (RAG) is an advanced framework in artificial intelligence that combines information retrieval with generative modeling to produce more accurate and contextually relevant responses.\\nIt bridges the gap between traditional search systems and generative models by integrating external knowledge sources into the generation process.\\nInstead of relying only on pre-trained parameters of a large language model, RAG actively queries a knowledge base or vector database during inference.\\nThis allows it to bring in the most up-to-date and domain-specific information that the model may not have memorized.\\nThe architecture typically involves two main components: a retriever and a generator.\\nThe retriever searches external documents, embeddings, or indexed corpora to fetch the most relevant passages.\\nThe generator then conditions on both the query and the retrieved content to create a coherent and factually grounded answer.\\nThis process ensures that the model is less prone to hallucinations and misinformation.\\nRAG is widely used in building enterprise chatbots, customer support systems, and domain-specific assistants.\\nFor example, in healthcare, it can access electronic health records or medical literature to provide compliance-friendly and accurate responses.\\nIn finance, it can pull the latest regulations or market data before generating recommendations.\\nThe retriever can be powered by dense vector embeddings using models like DPR or sentence-transformers.\\nThe generator is often a large language model such as BERT, GPT, or T5 adapted for conditioned text generation.\\nRAG also supports multi-turn conversations by maintaining retrieval memory across queries.\\nIts modular design allows organizations to plug in different retrieval systems or update knowledge bases without retraining the generator.\\nThis makes RAG more scalable and adaptive compared to static models.\\nIt also enhances transparency, as the retrieved sources can be displayed to justify the generated outputs.\\nRecent advancements combine RAG with multi-agent systems and LangChain pipelines for orchestration.\\nOverall, RAG represents a key step toward trustworthy, knowledge-grounded AI.\\nIt enables real-world deployment of generative models where accuracy, explainability, and compliance are critical.\\n\\nWould you like me to also **convert this into a simpler 5–6 line version** for quick interview use?\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "doc=DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "   \n",
    " loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=True\n",
    "\n",
    ")\n",
    "document=doc.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1e7cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "page_content=' machine learning is making the machines to learn itself. \n",
      "    Machine Learning (ML) is a branch of Artificial Intelligence that focuses on building systems capable of learning patterns from data and making predictions or decisions without being explicitly programmed. It involves training algorithms on historical datasets so they can identify trends, classify information, or forecast future outcomes. ML techniques range from simple models like linear regression to complex neural networks that power modern applications such as speech recognition, recommendation systems, fraud detection, and medical diagnostics. By continuously improving with more data, \n",
      "    ML enables businesses and researchers to uncover insights, automate processes, and solve problems at scale.' metadata={'source': 'data\\\\text_files\\\\ml.txt'}\n",
      "....................\n",
      "      ...........\n",
      "0 data\\text_files\\ml.txt\n",
      "1 data\\text_files\\rag.txt\n"
     ]
    }
   ],
   "source": [
    "print(len(document))\n",
    "print(document[0])\n",
    "print(\"\"\"....................\n",
    "      ...........\"\"\")\n",
    "for i,key in enumerate(document):\n",
    "    print(i,key.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a40c6",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f319d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['machine learning is making the machines to learn', 'to learn itself. \\n Machine Learning (ML) is a', '(ML) is a branch of Artificial Intelligence that', 'that focuses on building systems capable of', 'capable of learning patterns from data and making', 'and making predictions or decisions without being', 'being explicitly programmed. It involves training', 'training algorithms on historical datasets so they', 'so they can identify trends, classify information,', 'or forecast future outcomes. ML techniques range', 'range from simple models like linear regression to', 'to complex neural networks that power modern', 'modern applications such as speech recognition,', 'recommendation systems, fraud detection, and', 'and medical diagnostics. By continuously improving', 'improving with more data, \\n ML enables businesses', 'businesses and researchers to uncover insights,', 'insights, automate processes, and solve problems', 'problems at scale.']\n"
     ]
    }
   ],
   "source": [
    "# Character text splitter\n",
    "\n",
    "\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "\n",
    "text_to_split=document[0].page_content\n",
    "char_split= CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len #how to measure chunk size\n",
    "\n",
    ")\n",
    "\n",
    "chunks=char_split.split_text(text_to_split)\n",
    "print(type(chunks))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feefdc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " machine learning is making the machines to learn itself. \n",
      "    Machine Learning (ML) is a branch of Artificial Intelligence that focuses on building systems capable of learning patterns from data and making predictions or decisions without being explicitly programmed. It involves training algorithms on historical datasets so they can identify trends, classify information, or forecast future outcomes. ML techniques range from simple models like linear regression to complex neural networks that power modern applications such as speech recognition, recommendation systems, fraud detection, and medical diagnostics. By continuously improving with more data, \n",
      "    ML enables businesses and researchers to uncover insights, automate processes, and solve problems at scale.\n",
      "machine learning is making the machines to learn\n",
      "--------------\n",
      "to learn itself. \n",
      " Machine Learning (ML) is a\n",
      "--------------\n",
      "(ML) is a branch of Artificial Intelligence that\n",
      "--------------\n",
      "that focuses on building systems capable of\n",
      "--------------\n",
      "capable of learning patterns from data and making\n",
      "--------------\n",
      "and making predictions or decisions without being\n",
      "--------------\n",
      "being explicitly programmed. It involves training\n",
      "--------------\n",
      "training algorithms on historical datasets so they\n",
      "--------------\n",
      "so they can identify trends, classify information,\n",
      "--------------\n",
      "or forecast future outcomes. ML techniques range\n",
      "--------------\n",
      "range from simple models like linear regression to\n",
      "--------------\n",
      "to complex neural networks that power modern\n",
      "--------------\n",
      "modern applications such as speech recognition,\n",
      "--------------\n",
      "recommendation systems, fraud detection, and\n",
      "--------------\n",
      "and medical diagnostics. By continuously improving\n",
      "--------------\n",
      "improving with more data, \n",
      " ML enables businesses\n",
      "--------------\n",
      "businesses and researchers to uncover insights,\n",
      "--------------\n",
      "insights, automate processes, and solve problems\n",
      "--------------\n",
      "problems at scale.\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "print(text_to_split)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd867a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of chunks created are 9\n",
      "machine learning is making the machines to learn itself. \n",
      "    Machine Learning (ML) is a branch of\n",
      "____________\n",
      "branch of Artificial Intelligence that focuses on building systems capable of learning patterns\n",
      "____________\n",
      "patterns from data and making predictions or decisions without being explicitly programmed. It\n",
      "____________\n",
      "It involves training algorithms on historical datasets so they can identify trends, classify\n",
      "____________\n",
      "classify information, or forecast future outcomes. ML techniques range from simple models like\n",
      "____________\n",
      "like linear regression to complex neural networks that power modern applications such as speech\n",
      "____________\n",
      "as speech recognition, recommendation systems, fraud detection, and medical diagnostics. By\n",
      "____________\n",
      "By continuously improving with more data, \n",
      "    ML enables businesses and researchers to uncover\n",
      "____________\n",
      "uncover insights, automate processes, and solve problems at scale.\n",
      "____________\n"
     ]
    }
   ],
   "source": [
    "recsplit=RecursiveCharacterTextSplitter(\n",
    "    separators=[\"/n\",\" \"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "rec=recsplit.split_text(text_to_split)\n",
    "print(f\"no. of chunks created are {len(rec)}\")\n",
    "for chunk in rec:\n",
    "    print(chunk)\n",
    "    print(\"____________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "886acb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of chunks created are 9\n",
      "machine learning is making the machines to learn itself. \n",
      "    Machine Learning (ML) is a branch of\n",
      "____________\n",
      "branch of Artificial Intelligence that focuses on building systems capable of learning patterns\n",
      "____________\n",
      "patterns from data and making predictions or decisions without being explicitly programmed. It\n",
      "____________\n",
      "It involves training algorithms on historical datasets so they can identify trends, classify\n",
      "____________\n",
      "classify information, or forecast future outcomes. ML techniques range from simple models like\n",
      "____________\n",
      "like linear regression to complex neural networks that power modern applications such as speech\n",
      "____________\n",
      "as speech recognition, recommendation systems, fraud detection, and medical diagnostics. By\n",
      "____________\n",
      "By continuously improving with more data, \n",
      "    ML enables businesses and researchers to uncover\n",
      "____________\n",
      "uncover insights, automate processes, and solve problems at scale.\n",
      "____________\n"
     ]
    }
   ],
   "source": [
    "tokensplit=TokenTextSplitter(\n",
    "   \n",
    "    chunk_size=30,\n",
    "    chunk_overlap=5\n",
    "    \n",
    ")\n",
    "tok_split=tokensplit.split_text(text_to_split)\n",
    "print(f\"no. of chunks created are {len(rec)}\")\n",
    "for chunk in rec:\n",
    "    print(chunk)\n",
    "    print(\"____________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4de384",
   "metadata": {},
   "source": [
    "### PDF loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "334208fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m(\n\u001b[32m      2\u001b[39m     PyPDFLoader,\n\u001b[32m      3\u001b[39m     PyMuPDFLoader,\n\u001b[32m      4\u001b[39m     UnstructuredPDFLoader\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\__init__.py:740\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _module_lookup:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m         module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_module_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblob_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Blob\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdedoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DedocBaseLoader\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseImageBlobParser\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     _DEFAULT_PAGES_DELIMITER,\n\u001b[32m     34\u001b[39m     AmazonTextractPDFParser,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     PyPDFParser,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munstructured\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredFileLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\images.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatModel\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseBlobParser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\__init__.py:112\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr_name)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(attr_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    111\u001b[39m     module_name = _dynamic_imports.get(attr_name)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     result = \u001b[43mimport_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__spec__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mglobals\u001b[39m()[attr_name] = result\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\langchain_core\\_import_utils.py:36\u001b[39m, in \u001b[36mimport_attr\u001b[39m\u001b[34m(attr_name, module_name, package)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         module = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     38\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_llm_cache\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _normalize_messages\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     BaseLanguageModel,\n\u001b[32m     33\u001b[39m     LangSmithParams,\n\u001b[32m     34\u001b[39m     LanguageModelInput,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dumpd, dumps\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     38\u001b[39m     AIMessage,\n\u001b[32m     39\u001b[39m     AnyMessage,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     message_chunk_to_message,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:44\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMResult\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[32m     46\u001b[39m     _HAS_TRANSFORMERS = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     is_pretty_midi_available,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     33\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     is_flax_available,\n\u001b[32m     36\u001b[39m     is_mlx_available,\n\u001b[32m     37\u001b[39m     is_tf_available,\n\u001b[32m     38\u001b[39m     is_torch_available,\n\u001b[32m     39\u001b[39m     is_torch_fx_proxy,\n\u001b[32m     40\u001b[39m     requires,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     44\u001b[39m _CAN_RECORD_REGISTRY = {}\n\u001b[32m     47\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:197\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# For compressed_tensors, only check spec to allow compressed_tensors-nightly package\u001b[39;00m\n\u001b[32m    196\u001b[39m _compressed_tensors_available = importlib.util.find_spec(\u001b[33m\"\u001b[39m\u001b[33mcompressed_tensors\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m _pandas_available = \u001b[43m_is_package_available\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m _peft_available = _is_package_available(\u001b[33m\"\u001b[39m\u001b[33mpeft\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    199\u001b[39m _phonemizer_available = _is_package_available(\u001b[33m\"\u001b[39m\u001b[33mphonemizer\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:57\u001b[39m, in \u001b[36m_is_package_available\u001b[39m\u001b[34m(pkg_name, return_version)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m package_exists:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     51\u001b[39m         \u001b[38;5;66;03m# TODO: Once python 3.9 support is dropped, `importlib.metadata.packages_distributions()`\u001b[39;00m\n\u001b[32m     52\u001b[39m         \u001b[38;5;66;03m# should be used here to map from package name to distribution names\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m \n\u001b[32m     56\u001b[39m         \u001b[38;5;66;03m# Primary method to get the package version\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m         package_version = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib.metadata.PackageNotFoundError:\n\u001b[32m     59\u001b[39m         \u001b[38;5;66;03m# Fallback method: Only for \"torch\" and versions containing \"dev\"\u001b[39;00m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pkg_name == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:987\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(distribution_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    981\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m    982\u001b[39m \n\u001b[32m    983\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m    984\u001b[39m \u001b[33;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[33;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\RAG\\.venv\\Lib\\site-packages\\importlib_metadata\\__init__.py:557\u001b[39m, in \u001b[36mDistribution.version\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    556\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmd_none\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVersion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import(\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf3425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-02T17:14:39+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-02T17:14:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': 'Resume - Yella Reddy', 'trapped': '/False', 'source': 'data/pdfs/Yellareddy.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Nerella Yella Reddy\\nlinkedin.com/in/yella-reddy | yellareddy-portfolio.online | nerella@theworkmails.com | +1 4434946065\\nSUMMARY\\n• Data Scientist & AI/ML Engineer with 3.5+ YOE building and maintaining AI systems, LLMs, and NLPsolutions for\\nhealthcare, insurance, and enterprise platforms. Experienced in writingproduction-level Python code, prompt engineering,\\nand developing internal applications to support AI workflows. Skilled in REST API development, cloud platforms(AWS,\\nGCP , Azure), containerization with Docker, and delivering maintainable, testable solutions. Proven record of expanding\\nplatform functionality and improving efficiency through automation and scalable AI tools\\nSKILLS\\n• Programming Language/IDEs:Python, R Programming, SQL, Jupyter Notebook, Google Colab\\n• Machine Learning: Regression Models, Decision Trees, Random Forests, Naive Bayes, SVM, NumPy, Pandas, Matplotlib,\\nScikit-learn, TensorFlow, Keras, OpenCV , NLTK, XGBoost, PyTorch, Cohort Analysis, Hypothesis Testing\\n• AI & Deep Learning:CNN, ANN, RNN, ROBERTa, Claude, BERT, GPT-4, Named Entity Recognition (NER), Sentiment\\nAnalysis, Text Summarization, LangChain, Large Language Model (LLM), Retrieval-Augmented Generation (RAG)\\n• Cloud Platforms, MLOps & DevOps:AWS (S3, Glue, Redshift, Lambda, SageMaker, Bedrock), GCP (BigQuery, Cloud Run,\\nVertex AI), Azure (Data Factory, Synapse, Databricks), Docker, Kubernetes, Terraform,MLflow, FastAPI, Flask\\n• Database & Visualizations:SQL Server, PostgreSQL, MongoDB, Tableau, Power BI, Looker, Excel\\n• Data Engineering & Big Data:Apache Spark (PySpark, Spark Streaming), Hadoop (MapReduce, HDFS), Airflow, Kafka,\\nHive, Delta Lake\\n• Data Management & Governance:Data Cleaning, Data Wrangling, Data Transformation, Data Mapping, Data Modeling,\\nData Warehousing Concepts, Data Quality, Regulatory Compliance (GDPR, CCPA)\\nEXPERIENCE\\n• Cigna Health| Data Scientist | Saint Louis, USA Sep 2024 - Present\\n◦ Engineered secure AWS-based data pipelines(Glue, Redshift, S3) for real-time claims analytics and utilization tracking,\\ndelivered Tableau/SSRS dashboards, improving latency by 30–40%.\\n◦ Integrated EHR data with RAG modelsand deployed GenAI chatbotsusing Bedrock, boosting digital engagement and\\nimproving member self-service by 20% across care navigation.\\n◦ Automated HIPAA-compliant model deployment with CodePipeline, Docker, and GCP Kubernetes, ensured scalable,\\naudit-ready analytics workflows aligned with CMS and NCQA standards.\\n◦ Built high-performance PostgreSQL and Oracle databases to support claims, eligibility, and coordination workflows with\\nHIPAA-compliant design and optimized query performance.\\n◦ Developed and deployed ML models(Logistic Regression, Gradient Boosting, CNNs) under Agile for fraud detection,\\nreadmission risk, and population health, reducing healthcare data processing time by 25%.\\n◦ Designed AI/ML solutions via SageMaker, Bedrock, and LangChain for KED/readmission scoring, care planning, and\\nNLP-based medical note summarization to enhance care team productivity.\\n• Mphasis |AI/ML Engineer| Hyderabad, India Feb 2022 - Jul 2023\\n◦ Developed and deployed predictive analytics models using Random Forest and SVM for a leading banking client,\\nimproving loan default prediction accuracy by 19% and enabling proactive risk mitigation strategies.\\n◦ Engineered CNN and BERT-based NLPpipelines for sentiment analysis, text summarization, and NER on multi-lingual\\ncustomer feedback, increasing text classification precision to 94% and enhancing customer experience analytics.\\n◦ Conducted cohort analysis for retail customer segmentation, identifying high-value customer groups and boosting\\ntargeted campaign ROI.\\n◦ Implemented AWS-based data processing workflows for large-scale image and text datasets, reducing model training\\ntimes by 35% while ensuring data governance compliance.\\n◦ Developed XGBoost-based failure prediction models, reducing system downtime by 15%, and NLP pipelines using spaCy\\nand BERT for enhanced root-cause analysis and automated feedback classification, improving efficiency by 20%.\\n◦ Built computer vision solutions using OpenCV and TensorFlow for automated document verification, cutting manual\\nreview effort by 60% for insurance onboarding processes.\\n• HCL Tech| AI/ML Engineer| Chennai, India Jan 2021 - Jan 2022\\n◦ Developed and deployed regression, decision tree, and XGBoost modelsfor a European retail client, improving demand\\nforecasting accuracy by 21% and optimizing inventory allocation across various store locations.\\n◦ Implemented CNN-based image classification for product defect detection, achieving 92% accuracy and reducing manual\\ninspection workload by 35%.\\n◦ Built automated ETL pipelines for multi-source data (PostgreSQL, MongoDB, flat files) usingAzure services, cutting data\\npreparation time from 10 hours to under 3 hours per cycle.\\n◦ Designed and published Power BI dashboardswith DAX and Power Query to monitor sales trends, inventory levels, and\\nquality KPIs, enabling leadership to make data-backed decisions in real time.\\n◦ Performed extensive data cleaning, mapping, and quality checks on GDPR/CCPA-regulated datasets, ensuring 100%\\ncompliance with client data privacy and retention policies.\\nEDUCATION\\nMaster of Science in Computer Science| Southeast Missouri State University, GPA: 3.7/4.0, MO, USA May 2025\\nBachelor of Engineering in Computer Science & Engineering| VelTech University, GPA: 8.7/10, chennai, India May 2023')]\n",
      "Nerella Yella Reddy\n",
      "linkedin.com/in/yella-reddy | yellareddy-portfolio.online | nerella@theworkmails.com | +1 4434946065\n",
      "SUMMARY\n",
      "• Data Scientist & AI/ML Engineer with 3.5+ YOE building and maintaining AI systems, LLMs, and NLPsolutions for\n",
      "healthcare, insurance, and enterprise platforms. Experienced in writingproduction-level Python code, prompt engineering,\n",
      "and developing internal applications to support AI workflows. Skilled in REST API development, cloud platforms(AWS,\n",
      "GCP , Azure), containerization with Docker, and delivering maintainable, testable solutions. Proven record of expanding\n",
      "platform functionality and improving efficiency through automation and scalable AI tools\n",
      "SKILLS\n",
      "• Programming Language/IDEs:Python, R Programming, SQL, Jupyter Notebook, Google Colab\n",
      "• Machine Learning: Regression Models, Decision Trees, Random Forests, Naive Bayes, SVM, NumPy, Pandas, Matplotlib,\n",
      "Scikit-learn, TensorFlow, Keras, OpenCV , NLTK, XGBoost, PyTorch, Cohort Analysis, Hypothesis Testing\n",
      "• AI & Deep Learning:CNN, ANN, RNN, ROBERTa, Claude, BERT, GPT-4, Named Entity Recognition (NER), Sentiment\n",
      "Analysis, Text Summarization, LangChain, Large Language Model (LLM), Retrieval-Augmented Generation (RAG)\n",
      "• Cloud Platforms, MLOps & DevOps:AWS (S3, Glue, Redshift, Lambda, SageMaker, Bedrock), GCP (BigQuery, Cloud Run,\n",
      "Vertex AI), Azure (Data Factory, Synapse, Databricks), Docker, Kubernetes, Terraform,MLflow, FastAPI, Flask\n",
      "• Database & Visualizations:SQL Server, PostgreSQL, MongoDB, Tableau, Power BI, Looker, Excel\n",
      "• Data Engineering & Big Data:Apache Spark (PySpark, Spark Streaming), Hadoop (MapReduce, HDFS), Airflow, Kafka,\n",
      "Hive, Delta Lake\n",
      "• Data Management & Governance:Data Cleaning, Data Wrangling, Data Transformation, Data Mapping, Data Modeling,\n",
      "Data Warehousing Concepts, Data Quality, Regulatory Compliance (GDPR, CCPA)\n",
      "EXPERIENCE\n",
      "• Cigna Health| Data Scientist | Saint Louis, USA Sep 2024 - Present\n",
      "◦ Engineered secure AWS-based data pipelines(Glue, Redshift, S3) for real-time claims analytics and utilization tracking,\n",
      "delivered Tableau/SSRS dashboards, improving latency by 30–40%.\n",
      "◦ Integrated EHR data with RAG modelsand deployed GenAI chatbotsusing Bedrock, boosting digital engagement and\n",
      "improving member self-service by 20% across care navigation.\n",
      "◦ Automated HIPAA-compliant model deployment with CodePipeline, Docker, and GCP Kubernetes, ensured scalable,\n",
      "audit-ready analytics workflows aligned with CMS and NCQA standards.\n",
      "◦ Built high-performance PostgreSQL and Oracle databases to support claims, eligibility, and coordination workflows with\n",
      "HIPAA-compliant design and optimized query performance.\n",
      "◦ Developed and deployed ML models(Logistic Regression, Gradient Boosting, CNNs) under Agile for fraud detection,\n",
      "readmission risk, and population health, reducing healthcare data processing time by 25%.\n",
      "◦ Designed AI/ML solutions via SageMaker, Bedrock, and LangChain for KED/readmission scoring, care planning, and\n",
      "NLP-based medical note summarization to enhance care team productivity.\n",
      "• Mphasis |AI/ML Engineer| Hyderabad, India Feb 2022 - Jul 2023\n",
      "◦ Developed and deployed predictive analytics models using Random Forest and SVM for a leading banking client,\n",
      "improving loan default prediction accuracy by 19% and enabling proactive risk mitigation strategies.\n",
      "◦ Engineered CNN and BERT-based NLPpipelines for sentiment analysis, text summarization, and NER on multi-lingual\n",
      "customer feedback, increasing text classification precision to 94% and enhancing customer experience analytics.\n",
      "◦ Conducted cohort analysis for retail customer segmentation, identifying high-value customer groups and boosting\n",
      "targeted campaign ROI.\n",
      "◦ Implemented AWS-based data processing workflows for large-scale image and text datasets, reducing model training\n",
      "times by 35% while ensuring data governance compliance.\n",
      "◦ Developed XGBoost-based failure prediction models, reducing system downtime by 15%, and NLP pipelines using spaCy\n",
      "and BERT for enhanced root-cause analysis and automated feedback classification, improving efficiency by 20%.\n",
      "◦ Built computer vision solutions using OpenCV and TensorFlow for automated document verification, cutting manual\n",
      "review effort by 60% for insurance onboarding processes.\n",
      "• HCL Tech| AI/ML Engineer| Chennai, India Jan 2021 - Jan 2022\n",
      "◦ Developed and deployed regression, decision tree, and XGBoost modelsfor a European retail client, improving demand\n",
      "forecasting accuracy by 21% and optimizing inventory allocation across various store locations.\n",
      "◦ Implemented CNN-based image classification for product defect detection, achieving 92% accuracy and reducing manual\n",
      "inspection workload by 35%.\n",
      "◦ Built automated ETL pipelines for multi-source data (PostgreSQL, MongoDB, flat files) usingAzure services, cutting data\n",
      "preparation time from 10 hours to under 3 hours per cycle.\n",
      "◦ Designed and published Power BI dashboardswith DAX and Power Query to monitor sales trends, inventory levels, and\n",
      "quality KPIs, enabling leadership to make data-backed decisions in real time.\n",
      "◦ Performed extensive data cleaning, mapping, and quality checks on GDPR/CCPA-regulated datasets, ensuring 100%\n",
      "compliance with client data privacy and retention policies.\n",
      "EDUCATION\n",
      "Master of Science in Computer Science| Southeast Missouri State University, GPA: 3.7/4.0, MO, USA May 2025\n",
      "Bachelor of Engineering in Computer Science & Engineering| VelTech University, GPA: 8.7/10, chennai, India May 2023\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pypdf_load=PyPDFLoader(\"data/pdfs/Yellareddy.pdf\")\n",
    "except Exception as e:\n",
    "    print(f\"you got error : {e}\")\n",
    "\n",
    "pypdf=pypdf_load.load()\n",
    "print(type(pypdf))\n",
    "print(pypdf)\n",
    "print(pypdf[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a4cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nerella Yella Reddy\n",
      "linkedin.com/in/yella-reddy | yellareddy-portfolio.online | nerella@theworkmails.com | +1 4434946065\n",
      "SUMMARY\n",
      "• Data Scientist & AI/ML Engineer with 3.5+ YOE building and maintaining AI systems, LLMs, and NLP solutions for\n",
      "healthcare, insurance, and enterprise platforms. Experienced in writing production-level Python code, prompt engineering,\n",
      "and developing internal applications to support AI workflows. Skilled in REST API development, cloud platforms (AWS,\n",
      "GCP, Azure), containerization with Docker, and delivering maintainable, testable solutions. Proven record of expanding\n",
      "platform functionality and improving efficiency through automation and scalable AI tools\n",
      "SKILLS\n",
      "• Programming Language/IDEs: Python, R Programming, SQL, Jupyter Notebook, Google Colab\n",
      "• Machine Learning: Regression Models, Decision Trees, Random Forests, Naive Bayes, SVM, NumPy, Pandas, Matplotlib,\n",
      "Scikit-learn, TensorFlow, Keras, OpenCV, NLTK, XGBoost, PyTorch, Cohort Analysis, Hypothesis Testing\n",
      "• AI & Deep Learning: CNN, ANN, RNN, ROBERTa, Claude, BERT, GPT-4, Named Entity Recognition (NER), Sentiment\n",
      "Analysis, Text Summarization, LangChain, Large Language Model (LLM), Retrieval-Augmented Generation (RAG)\n",
      "• Cloud Platforms, MLOps & DevOps: AWS (S3, Glue, Redshift, Lambda, SageMaker, Bedrock), GCP (BigQuery, Cloud Run,\n",
      "Vertex AI), Azure (Data Factory, Synapse, Databricks), Docker, Kubernetes, Terraform,MLflow, FastAPI, Flask\n",
      "• Database & Visualizations: SQL Server, PostgreSQL, MongoDB, Tableau, Power BI, Looker, Excel\n",
      "• Data Engineering & Big Data: Apache Spark (PySpark, Spark Streaming), Hadoop (MapReduce, HDFS), Airflow, Kafka,\n",
      "Hive, Delta Lake\n",
      "• Data Management & Governance: Data Cleaning, Data Wrangling, Data Transformation, Data Mapping, Data Modeling,\n",
      "Data Warehousing Concepts, Data Quality, Regulatory Compliance (GDPR, CCPA)\n",
      "EXPERIENCE\n",
      "• Cigna Health | Data Scientist | Saint Louis, USA\n",
      "Sep 2024 - Present\n",
      "◦Engineered secure AWS-based data pipelines (Glue, Redshift, S3) for real-time claims analytics and utilization tracking,\n",
      "delivered Tableau/SSRS dashboards, improving latency by 30–40%.\n",
      "◦Integrated EHR data with RAG models and deployed GenAI chatbots using Bedrock, boosting digital engagement and\n",
      "improving member self-service by 20% across care navigation.\n",
      "◦Automated HIPAA-compliant model deployment with CodePipeline, Docker, and GCP Kubernetes, ensured scalable,\n",
      "audit-ready analytics workflows aligned with CMS and NCQA standards.\n",
      "◦Built high-performance PostgreSQL and Oracle databases to support claims, eligibility, and coordination workflows with\n",
      "HIPAA-compliant design and optimized query performance.\n",
      "◦Developed and deployed ML models (Logistic Regression, Gradient Boosting, CNNs) under Agile for fraud detection,\n",
      "readmission risk, and population health, reducing healthcare data processing time by 25%.\n",
      "◦Designed AI/ML solutions via SageMaker, Bedrock, and LangChain for KED/readmission scoring, care planning, and\n",
      "NLP-based medical note summarization to enhance care team productivity.\n",
      "• Mphasis |AI/ML Engineer | Hyderabad, India\n",
      "Feb 2022 - Jul 2023\n",
      "◦Developed and deployed predictive analytics models using Random Forest and SVM for a leading banking client,\n",
      "improving loan default prediction accuracy by 19% and enabling proactive risk mitigation strategies.\n",
      "◦Engineered CNN and BERT-based NLP pipelines for sentiment analysis, text summarization, and NER on multi-lingual\n",
      "customer feedback, increasing text classification precision to 94% and enhancing customer experience analytics.\n",
      "◦Conducted cohort analysis for retail customer segmentation, identifying high-value customer groups and boosting\n",
      "targeted campaign ROI.\n",
      "◦Implemented AWS-based data processing workflows for large-scale image and text datasets, reducing model training\n",
      "times by 35% while ensuring data governance compliance.\n",
      "◦Developed XGBoost-based failure prediction models, reducing system downtime by 15%, and NLP pipelines using spaCy\n",
      "and BERT for enhanced root-cause analysis and automated feedback classification, improving efficiency by 20%.\n",
      "◦Built computer vision solutions using OpenCV and TensorFlow for automated document verification, cutting manual\n",
      "review effort by 60% for insurance onboarding processes.\n",
      "• HCL Tech | AI/ML Engineer | Chennai, India\n",
      "Jan 2021 - Jan 2022\n",
      "◦Developed and deployed regression, decision tree, and XGBoost models for a European retail client, improving demand\n",
      "forecasting accuracy by 21% and optimizing inventory allocation across various store locations.\n",
      "◦Implemented CNN-based image classification for product defect detection, achieving 92% accuracy and reducing manual\n",
      "inspection workload by 35%.\n",
      "◦Built automated ETL pipelines for multi-source data (PostgreSQL, MongoDB, flat files) using Azure services, cutting data\n",
      "preparation time from 10 hours to under 3 hours per cycle.\n",
      "◦Designed and published Power BI dashboards with DAX and Power Query to monitor sales trends, inventory levels, and\n",
      "quality KPIs, enabling leadership to make data-backed decisions in real time.\n",
      "◦Performed extensive data cleaning, mapping, and quality checks on GDPR/CCPA-regulated datasets, ensuring 100%\n",
      "compliance with client data privacy and retention policies.\n",
      "EDUCATION\n",
      "Master of Science in Computer Science | Southeast Missouri State University, GPA: 3.7/4.0, MO, USA\n",
      "May 2025\n",
      "Bachelor of Engineering in Computer Science & Engineering | VelTech University, GPA: 8.7/10, chennai, India\n",
      "May 2023\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pymupdf_load=PyMuPDFLoader(\"data/pdfs/Yellareddy.pdf\")\n",
    "except Exception as e:\n",
    "    print(f\"you got error : {e}\")\n",
    "\n",
    "pymupdf=pymupdf_load.load()\n",
    "print(pymupdf[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a843d5",
   "metadata": {},
   "source": [
    "### doing everything in modular coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35690c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
